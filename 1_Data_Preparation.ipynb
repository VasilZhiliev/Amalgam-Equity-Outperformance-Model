{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext cudf.pandas\n",
    "# import cupy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pandas.tseries.offsets import MonthEnd, DateOffset\n",
    "import gc\n",
    "import numba\n",
    "import glob\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "numba.set_num_threads(8) #-----------> Change according to individual CPU thread count or potential compute power restrictions, this utilizes maximum cores\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'data\\\\permnos'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 500000\n",
    "last_permno = None\n",
    "temp_df = pd.DataFrame()\n",
    "\n",
    "# Read the CSV file in chunks due to insufficient memory\n",
    "for chunk in pd.read_csv('data/market_data_daily_full.csv', chunksize=chunk_size, low_memory=False):\n",
    "    if last_permno is None:\n",
    "        last_permno = chunk['PERMNO'].iloc[0]\n",
    "    curr_permno = chunk['PERMNO'].iloc[-1]\n",
    "\n",
    "\n",
    "    if curr_permno != last_permno:\n",
    "        temp_df = pd.concat([temp_df, chunk[chunk['PERMNO'] == last_permno]], ignore_index=True)\n",
    "        temp_df.to_pickle(f'{output_dir}/market_data_daily_{last_permno}.pkl')\n",
    "\n",
    "\n",
    "        for permno, group in chunk.groupby('PERMNO'):\n",
    "            if permno != last_permno and permno != curr_permno:\n",
    "                group.to_pickle(f'{output_dir}/market_data_daily_{permno}.pkl')\n",
    "        \n",
    "\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df = pd.concat([temp_df, chunk[chunk['PERMNO'] == curr_permno]], ignore_index=True)\n",
    "        last_permno = curr_permno\n",
    "    else:\n",
    "        temp_df = pd.concat([temp_df, chunk], ignore_index=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_data = pd.read_csv('data/birthdata.csv')\n",
    "acc_data_y = pd.read_csv('data/accounting_data_yearly.csv')\n",
    "acc_data_q = pd.read_csv('data/accounting_data_quarterly.csv')\n",
    "ff3_data_daily = pd.read_csv('data/ff3_data_daily.csv')\n",
    "sic_map = pd.read_csv(\"data/sic_amort.csv\", names=[\"SIC\", \"Amortization_Qtrs\"]).set_index(\"SIC\")[\"Amortization_Qtrs\"].to_dict()\n",
    "gind_map = pd.read_csv(\"data/gind_amort.csv\", names=[\"GIND\", \"Amortization_Qtrs\"]).set_index(\"GIND\")[\"Amortization_Qtrs\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permnos = []\n",
    "for filename in glob.glob('data\\\\permnos\\\\market_data_daily_*.pkl'):\n",
    "    permno = os.path.basename(filename).split('_')[-1].split('.')[0]\n",
    "    permnos.append(permno)\n",
    "\n",
    "permnos.sort()\n",
    "print(f\"Number of permnos: {len(permnos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set your correct native Windows path\n",
    "base_path = 'data\\\\permnos'\n",
    "\n",
    "ff3_data_daily['date'] = pd.to_datetime(ff3_data_daily['date'], format='%Y%m%d')\n",
    "\n",
    "def process_permno(permno):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    mkt_data = pd.read_pickle(file_path)\n",
    "\n",
    "    try:\n",
    "        mkt_data['date'] = pd.to_datetime(mkt_data['date'], format='%d/%m/%Y')\n",
    "    except ValueError:\n",
    "        mkt_data['date'] = pd.to_datetime(mkt_data['date'], format='%Y-%m-%d')\n",
    "\n",
    "    mkt_data = pd.merge(mkt_data, ff3_data_daily, on='date', how='inner')\n",
    "    mkt_data['RET'] = pd.to_numeric(mkt_data['RET'], errors='coerce')\n",
    "\n",
    "    first_valid_date = mkt_data[mkt_data['RET'].notna()]['date'].min()\n",
    "    mkt_data = mkt_data[mkt_data['date'] >= first_valid_date].copy()\n",
    "\n",
    "    mkt_data['excess_ret'] = mkt_data['RET'] - mkt_data['rf']\n",
    "    mkt_data['logRET'] = np.log1p(mkt_data['RET'])\n",
    "    mkt_data['logrf'] = np.log1p(mkt_data['rf'])\n",
    "    mkt_data['logexcess_ret'] = mkt_data['logRET'] - mkt_data['logrf']\n",
    "\n",
    "    mkt_data['quarter'] = mkt_data['date'].dt.to_period(\"Q\")\n",
    "    mkt_data['is_quarter_end'] = mkt_data.groupby(['PERMNO', 'quarter'])['date'].transform('max') == mkt_data['date']\n",
    "\n",
    "    mkt_data['logmktrf'] = np.log1p(mkt_data['mktrf'])\n",
    "    mkt_data['logsmb'] = np.log1p(mkt_data['smb'])\n",
    "    mkt_data['loghml'] = np.log1p(mkt_data['hml'])\n",
    "    mkt_data['logvwretd'] = np.log1p(mkt_data['vwretd'])\n",
    "    # Save to same path \n",
    "    mkt_data.to_pickle(file_path)\n",
    "\n",
    "\n",
    "Parallel(n_jobs=-1)(delayed(process_permno)(permno) for permno in permnos)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Sample Permno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_permno = 10001  # for example\n",
    "base_path = 'data\\\\permnos'\n",
    "file_path = os.path.join(base_path, f'market_data_daily_{sample_permno}.pkl')\n",
    "\n",
    "df = pd.read_pickle(file_path)\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FF3 Residual Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data\\\\permnos'\n",
    "\n",
    "def compute_resff3_12_1(permno):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PERMNO {permno}: {e}\")\n",
    "        return permno\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy()\n",
    "    df['resff3_12_1'] = np.nan\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if not row.get('is_quarter_end', False):\n",
    "            continue\n",
    "\n",
    "        current_date = row['date']\n",
    "        start_date = (current_date - DateOffset(months=12)).replace(day=1)\n",
    "        end_date = (current_date - DateOffset(months=1)).replace(day=1) + MonthEnd(0)\n",
    "\n",
    "        window = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "\n",
    "        if window['date'].dt.to_period('M').nunique() < 11:\n",
    "            continue\n",
    "\n",
    "        X = window[['logmktrf', 'logsmb', 'loghml']].fillna(0).to_numpy()\n",
    "        y = window['logexcess_ret'].fillna(0).to_numpy()\n",
    "\n",
    "        if len(X) == 0 or len(y) == 0:\n",
    "            continue\n",
    "\n",
    "        model = LinearRegression().fit(X, y)\n",
    "        y_hat = model.predict(X)\n",
    "        residual = y - y_hat\n",
    "        res_momentum = residual.sum()\n",
    "\n",
    "        df.loc[idx, 'resff3_12_1'] = res_momentum\n",
    "\n",
    "    df.to_pickle(file_path)\n",
    "    return permno\n",
    "\n",
    "\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_resff3_12_1)(permno) for permno in permnos\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -10 to -6 annualized returns; Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_seas_6_10an(permno):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PERMNO {permno}: {e}\")\n",
    "        return permno\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy()\n",
    "    df['seas_6_10an'] = np.nan\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if not row.get('is_quarter_end', False):\n",
    "            continue\n",
    "\n",
    "        current_date = row['date']\n",
    "        start_date = current_date - DateOffset(years=10)\n",
    "        end_date = current_date - DateOffset(years=6)\n",
    "\n",
    "        window = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "\n",
    "        if window['date'].dt.year.nunique() < 5:\n",
    "            continue\n",
    "\n",
    "        logrets = window['logRET'].fillna(0)\n",
    "        log_cum_return = logrets.sum()\n",
    "        log_ann_return = log_cum_return / 5\n",
    "\n",
    "        df.loc[idx, 'seas_6_10an'] = log_ann_return\n",
    "\n",
    "    df.to_pickle(file_path)\n",
    "    return permno\n",
    "\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_seas_6_10an)(permno) for permno in permnos\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -20 to -16 lagged annualized returns; Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_seas_20_16an(permno):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PERMNO {permno}: {e}\")\n",
    "        return permno\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy()\n",
    "    df['seas_20_16an'] = np.nan\n",
    "\n",
    "    dates = df['date'].values\n",
    "    is_qe = df['is_quarter_end'].values\n",
    "    logrets = df['logRET'].fillna(0).values\n",
    "\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        if not is_qe[idx]:\n",
    "            continue\n",
    "\n",
    "        current_date = row.date\n",
    "        start_date = current_date - DateOffset(years=20)\n",
    "        end_date = current_date - DateOffset(years=16)\n",
    "\n",
    "        mask = (dates >= np.datetime64(start_date)) & (dates <= np.datetime64(end_date))\n",
    "        if pd.Series(dates[mask]).dt.year.nunique() < 5:\n",
    "            continue\n",
    "\n",
    "        log_cum_return = logrets[mask].sum()\n",
    "        log_ann_return = log_cum_return / 5\n",
    "\n",
    "        df.iat[idx, df.columns.get_loc('seas_20_16an')] = log_ann_return\n",
    "\n",
    "    df.to_pickle(file_path)\n",
    "    return permno\n",
    "\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_seas_20_16an)(permno) for permno in permnos\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -10 to -6 non-annualized returns; Seasonality; Low Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_seas_6_10na(permno):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PERMNO {permno}: {e}\")\n",
    "        return permno\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy()\n",
    "    df['seas_6_10na'] = np.nan\n",
    "\n",
    "    dates = df['date'].values\n",
    "    is_qe = df['is_quarter_end'].values\n",
    "    logrets = df['logRET'].fillna(0).values\n",
    "\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        if not is_qe[idx]:\n",
    "            continue\n",
    "\n",
    "        current_date = row.date\n",
    "        start_date = current_date - DateOffset(years=10)\n",
    "        end_date = current_date - DateOffset(years=6)\n",
    "\n",
    "        mask = (dates >= np.datetime64(start_date)) & (dates <= np.datetime64(end_date))\n",
    "        if pd.Series(dates[mask]).dt.year.nunique() < 5:\n",
    "            continue\n",
    "\n",
    "        log_cum_return = logrets[mask].sum()\n",
    "        df.iat[idx, df.columns.get_loc('seas_6_10na')] = log_cum_return\n",
    "\n",
    "    df.to_pickle(file_path)\n",
    "    return permno\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_seas_6_10na)(permno) for permno in permnos\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -15 to -11 non-annualized returns; Seasonality; Low Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_seas_11_15na(permno):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PERMNO {permno}: {e}\")\n",
    "        return permno\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy()\n",
    "    df['seas_11_15na'] = np.nan\n",
    "\n",
    "    dates = df['date'].values\n",
    "    is_qe = df['is_quarter_end'].values\n",
    "    logrets = df['logRET'].fillna(0).values\n",
    "\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        if not is_qe[idx]:\n",
    "            continue\n",
    "\n",
    "        current_date = row.date\n",
    "        start_date = current_date - DateOffset(years=15)\n",
    "        end_date = current_date - DateOffset(years=11)\n",
    "\n",
    "        mask = (dates >= np.datetime64(start_date)) & (dates <= np.datetime64(end_date))\n",
    "        if pd.Series(dates[mask]).dt.year.nunique() < 5:\n",
    "            continue\n",
    "\n",
    "        log_cum_return = logrets[mask].sum()\n",
    "        df.iat[idx, df.columns.get_loc('seas_11_15na')] = log_cum_return\n",
    "\n",
    "    df.to_pickle(file_path)\n",
    "    return permno\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_seas_11_15na)(permno) for permno in permnos\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Trading Days 252 window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zero_trades_252(permno):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PERMNO {permno}: {e}\")\n",
    "        return permno\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy()\n",
    "    df['zero_trades_252'] = pd.NA\n",
    "\n",
    "    dates = df['date'].values\n",
    "    is_qe = df['is_quarter_end'].values\n",
    "    vol = df['VOL'].fillna(0).values\n",
    "\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        if not is_qe[idx]:\n",
    "            continue\n",
    "\n",
    "        current_date = row.date\n",
    "        start_date = current_date - DateOffset(years=1)\n",
    "\n",
    "        mask = (dates >= np.datetime64(start_date)) & (dates < np.datetime64(current_date))\n",
    "        if pd.Series(dates[mask]).dt.to_period('M').nunique() < 12:\n",
    "            continue\n",
    "\n",
    "        zero_days = (vol[mask] == 0).sum()\n",
    "        df.iat[idx, df.columns.get_loc('zero_trades_252')] = zero_days\n",
    "\n",
    "    df.to_pickle(file_path)\n",
    "    return permno\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_zero_trades_252)(permno) for permno in permnos\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Trading Days 21 window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zero_trades_21(permno):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PERMNO {permno}: {e}\")\n",
    "        return permno\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy()\n",
    "    df['zero_trades_21'] = pd.NA\n",
    "\n",
    "    dates = df['date'].values\n",
    "    is_qe = df['is_quarter_end'].values\n",
    "    vol = df['VOL'].fillna(0).values\n",
    "\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        if not is_qe[idx]:\n",
    "            continue\n",
    "\n",
    "        current_date = row.date\n",
    "        start_date = current_date - pd.DateOffset(months=1)\n",
    "\n",
    "        mask = (dates >= np.datetime64(start_date)) & (dates < np.datetime64(current_date))\n",
    "\n",
    "        # Require that at least 1 full month of data is present\n",
    "        if pd.Series(dates[mask]).dt.to_period('M').nunique() < 1:\n",
    "            continue\n",
    "\n",
    "        zero_days = (vol[mask] == 0).sum()\n",
    "        df.iat[idx, df.columns.get_loc('zero_trades_21')] = zero_days\n",
    "\n",
    "    df.to_pickle(file_path)\n",
    "    return permno\n",
    "\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_zero_trades_21)(permno) for permno in permnos\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FF3 21-day Idiosyncratic Volatility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ivol_ff3_21d(permno):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PERMNO {permno}: {e}\")\n",
    "        return permno\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy()\n",
    "    df['month'] = df['date'].dt.to_period('M')\n",
    "    df['ivol_ff3_21d'] = np.nan\n",
    "\n",
    "    # precompute grouped structure once\n",
    "    grouped = dict(tuple(df.groupby('month')))\n",
    "\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        if not row.is_quarter_end:\n",
    "            continue\n",
    "\n",
    "        current_month = row.date.to_period('M')\n",
    "        monthly_data = grouped.get(current_month)\n",
    "\n",
    "        if monthly_data is None or len(monthly_data) < 15:\n",
    "            continue\n",
    "\n",
    "        monthly_data = monthly_data.dropna(subset=['logRET', 'logrf', 'mktrf', 'smb', 'hml'])\n",
    "        if len(monthly_data) < 15:\n",
    "            continue\n",
    "\n",
    "        logexcess = monthly_data['logRET'].values - monthly_data['logrf'].values\n",
    "        X = monthly_data[['logmktrf', 'logsmb', 'loghml']].values\n",
    "\n",
    "        try:\n",
    "            model = LinearRegression().fit(X, logexcess)\n",
    "            residuals = logexcess - model.predict(X)\n",
    "            ivol = np.std(residuals)\n",
    "            df.iat[idx, df.columns.get_loc('ivol_ff3_21d')] = ivol\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    df.to_pickle(file_path)\n",
    "    return permno\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_ivol_ff3_21d)(permno) for permno in permnos\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firm Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"birth_data columns:\", birth_data.columns.tolist())\n",
    "print(birth_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data\\\\permnos'\n",
    "\n",
    "birth_data = pd.read_csv(\"data/birthdata.csv\")\n",
    "birth_data.columns = birth_data.columns.str.strip().str.upper()\n",
    "birth_data['BEGDAT'] = pd.to_datetime(birth_data['BEGDAT'], dayfirst=True)\n",
    "\n",
    "def compute_firm_age(permno, birth_data):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception:\n",
    "        return permno  \n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "    # Merge on PERMNO (not PERMCO)\n",
    "    df = df.merge(birth_data[['PERMNO', 'BEGDAT']], on='PERMNO', how='left')\n",
    "\n",
    "    if 'BEGDAT' not in df.columns:\n",
    "        return permno\n",
    "\n",
    "    days_alive = (df['date'] - df['BEGDAT']).dt.days\n",
    "    df['firm_age'] = round((days_alive / 365) * 4) / 4\n",
    "    df.loc[~df['is_quarter_end'], 'firm_age'] = pd.NA\n",
    "\n",
    "    df.to_pickle(file_path)\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    return permno\n",
    "\n",
    "batch_size = 500  # You can tweak this according to processing power and memory\n",
    "num_batches = (len(permnos) + batch_size - 1) // batch_size\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch = permnos[i * batch_size : (i + 1) * batch_size]\n",
    "    Parallel(n_jobs=-1)(\n",
    "        delayed(compute_firm_age)(permno, birth_data) for permno in batch\n",
    "    )\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Jegadeesh & Titman 12-1 MOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mom_12_1(permno):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PERMNO {permno}: {e}\")\n",
    "        return permno\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy()\n",
    "    df['mom_12_1'] = np.nan\n",
    "\n",
    "    dates = df['date'].values\n",
    "    is_qe = df['is_quarter_end'].values\n",
    "    logrets = df['logRET'].fillna(0).values\n",
    "\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        if not is_qe[idx]:\n",
    "            continue\n",
    "\n",
    "        current_date = row.date\n",
    "        start_date = (current_date - DateOffset(months=12)).replace(day=1)\n",
    "        end_date = (current_date - DateOffset(months=1)).replace(day=1) + MonthEnd(0)\n",
    "\n",
    "        mask = (dates >= np.datetime64(start_date)) & (dates <= np.datetime64(end_date))\n",
    "        if pd.Series(dates[mask]).dt.to_period('M').nunique() < 11:\n",
    "            continue\n",
    "\n",
    "        mom = logrets[mask].sum()\n",
    "        df.iat[idx, df.columns.get_loc('mom_12_1')] = mom\n",
    "\n",
    "    df.to_pickle(file_path)\n",
    "    return permno\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_mom_12_1)(permno) for permno in permnos\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest 5 days of returns scaled by monthly volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmax5_rvol_21d(permno):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PERMNO {permno}: {e}\")\n",
    "        return permno\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy()\n",
    "    df['rmax5_rvol_21d'] = np.nan\n",
    "\n",
    "    dates = df['date'].values\n",
    "    is_qe = df['is_quarter_end'].values\n",
    "    logrets = df['logRET'].values\n",
    "\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        if not is_qe[idx]:\n",
    "            continue\n",
    "\n",
    "        current_date = row.date\n",
    "        month_start = (current_date - DateOffset(months=1)).replace(day=1)\n",
    "        month_end = current_date - MonthEnd(1)\n",
    "\n",
    "        mask = (dates >= np.datetime64(month_start)) & (dates <= np.datetime64(month_end))\n",
    "        rets = pd.Series(logrets[mask]).dropna()\n",
    "\n",
    "        if len(rets) < 5:\n",
    "            continue\n",
    "\n",
    "        top5_sum = rets.abs().nlargest(5).sum()\n",
    "        vol = rets.std()\n",
    "\n",
    "        if vol > 0:\n",
    "            rmax5_rvol = top5_sum / (np.sqrt(252) * vol)\n",
    "            df.iat[idx, df.columns.get_loc('rmax5_rvol_21d')] = rmax5_rvol\n",
    "\n",
    "    df.to_pickle(file_path)\n",
    "    return permno\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_rmax5_rvol_21d)(permno) for permno in permnos\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-month Share Turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_turnover_126d(permno):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PERMNO {permno}: {e}\")\n",
    "        return permno\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy()\n",
    "    df['turnover_126d'] = np.nan\n",
    "\n",
    "    dates = df['date'].values\n",
    "    is_qe = df['is_quarter_end'].values\n",
    "    vol = df['VOL'].values\n",
    "    shrout = df['SHROUT'].values\n",
    "\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        if not is_qe[idx]:\n",
    "            continue\n",
    "\n",
    "        current_date = row.date\n",
    "        start_date = current_date - DateOffset(months=6)\n",
    "\n",
    "        mask = (dates >= np.datetime64(start_date)) & (dates < np.datetime64(current_date))\n",
    "        if pd.Series(dates[mask]).dt.to_period(\"M\").nunique() < 6:\n",
    "            continue\n",
    "\n",
    "        vol_sum = vol[mask].sum()\n",
    "\n",
    "        valid_shrout = shrout[mask]\n",
    "        shrout_start = valid_shrout[0] if len(valid_shrout) > 0 else np.nan\n",
    "\n",
    "        turnover = vol_sum / shrout_start if shrout_start != 0 else np.nan\n",
    "        df.iat[idx, df.columns.get_loc('turnover_126d')] = turnover\n",
    "\n",
    "    df.to_pickle(file_path)\n",
    "    return permno\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(compute_turnover_126d)(permno) for permno in permnos\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_market_cap(permno):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PERMNO {permno}: {e}\")\n",
    "        return permno\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy()\n",
    "\n",
    "    if 'PRC' not in df.columns or 'SHROUT' not in df.columns or 'is_quarter_end' not in df.columns:\n",
    "        return permno\n",
    "\n",
    "    prc = df['PRC'].abs().values  \n",
    "    shrout = df['SHROUT'].values\n",
    "    is_qe = df['is_quarter_end'].values\n",
    "\n",
    "\n",
    "    mc = np.full(len(df), np.nan)\n",
    "\n",
    "    # Calculate market cap only at quarter-end\n",
    "    mc[is_qe] = prc[is_qe] * shrout[is_qe]\n",
    "\n",
    "    df['mc'] = mc\n",
    "\n",
    "    df.to_pickle(file_path)\n",
    "    return permno\n",
    "Parallel(n_jobs=-1)(delayed(compute_market_cap)(permno) for permno in permnos)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Sample Permno Again. All variables defined above should be present as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'data\\\\market_data_daily_sample_10001.csv'\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation of data into a main market variable file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def aggregate_quarterly_for_permno(permno, output_dir):\n",
    "    file_path = os.path.join(base_path, f'market_data_daily_{permno}.pkl')\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load PERMNO {permno}: {e}\")\n",
    "        return\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').copy()\n",
    "    df['quarter'] = df['date'].dt.to_period('Q')\n",
    "    df['is_quarter_end'] = df.groupby('quarter')['date'].transform('max') == df['date']\n",
    "\n",
    "    if 'logRET' not in df.columns:\n",
    "        return  # skip if missing core columns\n",
    "\n",
    "    qagg = (\n",
    "        df.groupby('quarter').agg({\n",
    "            'logRET': 'sum',\n",
    "            'logvwretd': 'sum',\n",
    "            'VOL': 'sum',\n",
    "            'SHROUT': 'last',\n",
    "            'resff3_12_1': 'last',\n",
    "            'zero_trades_252': 'last',\n",
    "            'zero_trades_21': 'last',\n",
    "            'turnover_126d': 'last',\n",
    "            'ivol_ff3_21d': 'last',\n",
    "            'rmax5_rvol_21d': 'last',\n",
    "            'mom_12_1': 'last',\n",
    "            'firm_age': 'last',\n",
    "            'seas_6_10an': 'last',\n",
    "            'seas_6_10na': 'last',\n",
    "            'seas_11_15na': 'last',\n",
    "            'seas_20_16an': 'last',\n",
    "            'mc': 'last',\n",
    "            'logmktrf': 'sum',\n",
    "            'logsmb': 'sum',\n",
    "            'loghml': 'sum',\n",
    "            'logrf': 'sum'\n",
    "        }).reset_index()\n",
    "    )\n",
    "    \n",
    "\n",
    "    qagg['RET'] = np.exp(qagg['logRET']) - 1\n",
    "    qagg['vwretd'] = np.exp(qagg['logvwretd']) - 1\n",
    "    qagg['mktrf'] = np.exp(qagg['logmktrf']) - 1\n",
    "    qagg['smb'] = np.exp(qagg['logsmb']) - 1\n",
    "    qagg['hml'] = np.exp(qagg['loghml']) - 1\n",
    "    qagg['rf'] = np.exp(qagg['logrf']) - 1\n",
    "\n",
    "    # Attach PERMNO + quarter-end date\n",
    "    end_dates = df[df['is_quarter_end']][['quarter', 'date']].drop_duplicates().rename(columns={'date': 'qtr_end_date'})\n",
    "    qagg = qagg.merge(end_dates, on='quarter', how='left')\n",
    "    qagg['PERMNO'] = permno\n",
    "\n",
    "    output_path = os.path.join(output_dir, f'quarterly_{permno}.feather')\n",
    "    qagg.to_feather(output_path)\n",
    "output_dir = r'D:\\Master Thesis\\Code\\quarterly_aggregated'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "Parallel(n_jobs=6)(\n",
    "    delayed(aggregate_quarterly_for_permno)(permno, output_dir)\n",
    "    for permno in tqdm(permnos, desc=\"Processing PERMNOs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation of all market files into one quarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob(os.path.join(output_dir, 'quarterly_*.feather'))\n",
    "df_list = [pd.read_feather(f) for f in files]\n",
    "full_quarterly = pd.concat(df_list, ignore_index=True)\n",
    "full_quarterly.to_csv(r'D:\\Master Thesis\\Code\\mkt_data_quarterly_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backfilling historical GIND values according to their future classification first, and their SIC classification second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_data_q['sic'] = pd.to_numeric(acc_data_q['sic'], errors='coerce').astype('Int64')\n",
    "acc_data_q['gind'] = pd.to_numeric(acc_data_q['gind'], errors='coerce').astype('Int64')\n",
    "\n",
    "\n",
    "#Impute from self-history\n",
    "\n",
    "# Get most common known GIND per firm PERMNO\n",
    "firm_gind_map = (\n",
    "    acc_data_q[acc_data_q['gind'].notna()]\n",
    "    .groupby('LPERMNO')['gind']\n",
    "    .agg(lambda x: x.value_counts().index[0])\n",
    "    .reset_index()\n",
    "    .rename(columns={'gind': 'gind_from_self'})\n",
    ")\n",
    "\n",
    "# Merge into main data\n",
    "acc_data_q = acc_data_q.merge(firm_gind_map, on='LPERMNO', how='left')\n",
    "\n",
    "# Flag and fill\n",
    "acc_data_q['gind_self_flag'] = acc_data_q['gind'].isna() & acc_data_q['gind_from_self'].notna()\n",
    "acc_data_q['gind'] = acc_data_q['gind'].fillna(acc_data_q['gind_from_self'])\n",
    "acc_data_q = acc_data_q.drop(columns=['gind_from_self'])\n",
    "\n",
    "\n",
    "#Impute from SIC-level peers\n",
    "\n",
    "# Build from SIC code to most common GIND code from all known GINDs\n",
    "sic_to_gind_map = (\n",
    "    acc_data_q[acc_data_q['gind'].notna()]\n",
    "    .groupby('sic')['gind']\n",
    "    .agg(lambda x: x.value_counts().index[0])\n",
    "    .reset_index()\n",
    "    .rename(columns={'gind': 'gind_from_sic'})\n",
    ")\n",
    "\n",
    "acc_data_q = acc_data_q.merge(sic_to_gind_map, on='sic', how='left')\n",
    "\n",
    "acc_data_q['gind_sic_flag'] = acc_data_q['gind'].isna() & acc_data_q['gind_from_sic'].notna()\n",
    "acc_data_q['gind'] = acc_data_q['gind'].fillna(acc_data_q['gind_from_sic'])\n",
    "\n",
    "acc_data_q = acc_data_q.drop(columns=['gind_from_sic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing GINDs:\", acc_data_q['gind'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeliness of 10-Q Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_data_q['pdateq'] = pd.to_datetime(acc_data_q['pdateq'], errors='coerce')\n",
    "acc_data_q['rdq'] = pd.to_datetime(acc_data_q['rdq'], errors='coerce')\n",
    "acc_data_q['datadate'] = pd.to_datetime(acc_data_q['datadate'], errors='coerce')\n",
    "\n",
    "pdate_ok = acc_data_q['pdateq'].notna() & (acc_data_q['pdateq'] <= acc_data_q['datadate'])\n",
    "rdq_ok = acc_data_q['rdq'].notna() & (acc_data_q['rdq'] <= acc_data_q['datadate'])\n",
    "update_final = acc_data_q['updq'] == 3\n",
    "update_prelim = acc_data_q['updq'] == 2\n",
    "\n",
    "total = len(acc_data_q)\n",
    "timely_pdate = pdate_ok.sum()\n",
    "timely_rdq_final = ((rdq_ok) & update_final).sum()\n",
    "rdq_prelim = ((rdq_ok) & update_prelim).sum()\n",
    "missing_rdq_pdate = ((~pdate_ok) & (~rdq_ok)).sum()\n",
    "\n",
    "print(f\"Total rows: {total:,}\")\n",
    "print(f\"PDATEQ timely: {timely_pdate:,} ({timely_pdate / total:.2%})\")\n",
    "print(f\"RDQ timely and final (updq=3): {timely_rdq_final:,} ({timely_rdq_final / total:.2%})\")\n",
    "print(f\"RDQ timely but preliminary (updq=2): {rdq_prelim:,} ({rdq_prelim / total:.2%})\")\n",
    "print(f\"Missing or delayed RDQ and PDATEQ: {missing_rdq_pdate:,} ({missing_rdq_pdate / total:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flag potentially late fillings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean masks\n",
    "pdate_ok = acc_data_q['pdateq'].notna() & (acc_data_q['pdateq'] <= acc_data_q['datadate'])\n",
    "rdq_ok = acc_data_q['rdq'].notna() & (acc_data_q['rdq'] <= acc_data_q['datadate'])\n",
    "update_final = acc_data_q['updq'] == 3\n",
    "update_prelim = acc_data_q['updq'] == 2\n",
    "\n",
    "# Delayed or missing reporting (requires shift by 2)\n",
    "delayed_mask = (~pdate_ok) & (~rdq_ok | ~(update_final | update_prelim))\n",
    "\n",
    "# Timely data (shift by 1)\n",
    "timely_mask = ~delayed_mask\n",
    "\n",
    "# Add to df for later merging with acc_data\n",
    "acc_data_q['delay_flag'] = delayed_mask.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge yearly and quarterly accounting data ###\n",
    "acc_data_q.rename(columns={'fyearq': 'y', 'fqtr': 'qtr'}, inplace=True)\n",
    "acc_data_y.rename(columns={'fyear': 'y'}, inplace=True)\n",
    "acc_data = pd.merge(acc_data_q, acc_data_y[['LPERMNO', 'y', 'pstkrv', 'pstkl']], on=['LPERMNO', 'y'], how='left')\n",
    "\n",
    "acc_data.rename(columns={'LPERMNO': 'PERMNO'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert year-to-date accounting data to quarterly data ###\n",
    "def ytd_to_qtr(data, col):\n",
    "    colq = col + '_q'\n",
    "    data[colq] = data.groupby(['PERMNO', 'y'])[col].diff()\n",
    "    data[colq] = data[colq].fillna(data[col])\n",
    "\n",
    "ytd_cols = ['oancfy', 'scstkcy', 'sstky', 'wcapchy']\n",
    "for col in ytd_cols:\n",
    "    ytd_to_qtr(acc_data, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GICS and SIC Industry classification with their respective amortization life in quarters\n",
    "acc_data['GIND'] = acc_data['gind'].fillna(0).astype(int)\n",
    "acc_data['SIC'] = acc_data['sic'].fillna(0).astype(int)\n",
    "\n",
    "def get_classification(row):\n",
    "    if row['GIND'] != 0:\n",
    "        return ('GIND', row['GIND'])\n",
    "    else:\n",
    "        return ('SIC', row['SIC'])\n",
    "\n",
    "acc_data['industry_class'] = acc_data.apply(get_classification, axis=1)\n",
    "\n",
    "# GIND as industry code — 0 means 'unknown', exclude those later\n",
    "acc_data['industry_code'] = acc_data['GIND']\n",
    "acc_data.loc[acc_data['industry_code'] == 0, 'industry_code'] = np.nan\n",
    "\n",
    "# Convert to categorical \n",
    "acc_data['industry_cat'] = acc_data['industry_code'].astype('category')\n",
    "\n",
    "# R&D amortization\n",
    "acc_data['R&D'] = acc_data['xrdq'].fillna(0)\n",
    "\n",
    "def rd_ammort(group: pd.DataFrame, kind: str, code: int) -> pd.DataFrame:\n",
    "    if kind == 'GIND':\n",
    "        rd_qtrs = gind_map.get(code, 12)\n",
    "    else:\n",
    "        rd_qtrs = sic_map.get(code, 12)\n",
    "\n",
    "    group = group[['PERMNO', 'y', 'qtr', 'R&D']]\n",
    "    group = group.sort_values(['PERMNO', 'y', 'qtr'])\n",
    "    group['R&D_ammort'] = (\n",
    "        group.groupby('PERMNO')['R&D']\n",
    "        .shift(1, fill_value=0)\n",
    "        .rolling(window=rd_qtrs, min_periods=1)\n",
    "        .sum() / rd_qtrs\n",
    "    )\n",
    "    return group\n",
    "\n",
    "# Apply amortization logic\n",
    "tmp = acc_data.groupby('industry_class', group_keys=False).apply(lambda g: rd_ammort(g, *g.name))\n",
    "\n",
    "# Merge back\n",
    "acc_data = acc_data.merge(tmp[['PERMNO', 'y', 'qtr', 'R&D_ammort']], on=['PERMNO', 'y', 'qtr'], how='left')\n",
    "acc_data['R&D cum'] = acc_data.groupby('PERMNO')['R&D'].cumsum()\n",
    "acc_data['R&D Ammort cum'] = acc_data.groupby('PERMNO')['R&D_ammort'].cumsum()\n",
    "acc_data['R&D cap'] = acc_data['R&D cum'] - acc_data['R&D Ammort cum']\n",
    "acc_data['R&D cap'] = acc_data['R&D cap'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate accounting features ###\n",
    "fin = acc_data['sic'].between(6000, 6999)  # Financial SIC codes\n",
    "util = acc_data['sic'].between(4900, 4999) # Utilities SIC codes\n",
    "\n",
    "\n",
    "# Income Statement Items\n",
    "\n",
    "# Revenue\n",
    "acc_data['SALES'] = acc_data['saleq'].fillna(acc_data['revtq'])  # trust 0s as valid\n",
    "\n",
    "# Financial firms: revtq → saleq → finrevq \n",
    "acc_data.loc[fin, 'SALES'] = (\n",
    "    acc_data.loc[fin, 'revtq']\n",
    "    .fillna(acc_data.loc[fin, 'saleq'])\n",
    "    .fillna(acc_data.loc[fin, 'finrevq'].fillna(0))\n",
    ")\n",
    "\n",
    "# Remove R&D Expense from Selling, General and Administrative Expenses and from Operating Expenses\n",
    "acc_data['xsgaq'] = acc_data['xsgaq'] - acc_data['R&D'].fillna(0)\n",
    "acc_data['xoprq'] = acc_data['xoprq'] - acc_data['R&D'].fillna(0)\n",
    "\n",
    "# COGS\n",
    "acc_data['COGS'] = acc_data['cogsq'].fillna(acc_data['xoprq'] - acc_data['xsgaq'])\n",
    "\n",
    "acc_data.loc[fin, 'COGS'] = (\n",
    "    acc_data.loc[fin, 'cogsq']\n",
    "    .fillna(acc_data.loc[fin, 'xoprq'] - acc_data.loc[fin, 'xsgaq'])\n",
    "    .fillna(acc_data.loc[fin, 'finxoprq'] - acc_data.loc[fin, 'xsgaq'])\n",
    "    .fillna(acc_data.loc[fin, 'xintq'] + acc_data.loc[fin, 'xsgaq'])\n",
    "    .fillna(acc_data.loc[fin, 'finxintq'] + acc_data.loc[fin, 'xsgaq']).fillna(0)  \n",
    ")\n",
    "\n",
    "                                                              \n",
    "# Gross Profit\n",
    "acc_data['GP'] = acc_data['SALES'] - acc_data['COGS']\n",
    "\n",
    "\n",
    "# Operating Expenses\n",
    "acc_data['OPEX'] = acc_data['xoprq'].fillna(acc_data['cogsq'] + acc_data['xsgaq'])\n",
    "\n",
    "acc_data.loc[fin, 'OPEX'] = (acc_data.loc[fin, 'xoprq'].fillna(acc_data.loc[fin, 'cogsq'] + acc_data.loc[fin, 'xsgaq'])\n",
    "    .fillna(acc_data.loc[fin, 'finxoprq'])  \n",
    ")\n",
    "\n",
    "# EBITDA\n",
    "acc_data['EBITDA'] = acc_data['oibdpq'] + acc_data['R&D']\n",
    "acc_data['EBITDA'] = acc_data['EBITDA'].fillna(acc_data['SALES'] - acc_data['OPEX'] + acc_data['R&D'])\n",
    "\n",
    "acc_data.loc[util, 'EBITDA'] = (\n",
    "    acc_data.loc[util, 'oibdpq']\n",
    "    .fillna(acc_data.loc[util, 'uopiq'])\n",
    "    .fillna(acc_data.loc[util, 'SALES'] - acc_data.loc[util, 'OPEX'] + acc_data.loc[util, 'R&D'])\n",
    ")\n",
    "\n",
    "# Regular reported D&A\n",
    "acc_data['D&A'] = acc_data['dpq'].fillna(0)\n",
    "\n",
    "# EBIT\n",
    "acc_data['EBIT'] = acc_data['oiadpq'] + acc_data['R&D'] - acc_data['R&D_ammort']\n",
    "acc_data['EBIT'] = acc_data['EBIT'].fillna(acc_data['EBITDA'] - acc_data['dpq'] - acc_data['R&D_ammort'])\n",
    "\n",
    "# Net Income\n",
    "acc_data['NI'] = acc_data['ibq']\n",
    "acc_data['NI'] = acc_data['NI'].fillna(acc_data['niq'])\n",
    "acc_data['NI'] = acc_data['NI'] + acc_data['R&D'] - acc_data['R&D_ammort']\n",
    "\n",
    "\n",
    "\n",
    "# Balance Sheet Items\n",
    "\n",
    "# Cash + ST Investments\n",
    "acc_data['CASHST'] = acc_data['cheq']\n",
    "\n",
    "# Inventory\n",
    "acc_data['INVENTORY'] = acc_data['invtq']\n",
    "acc_data.loc[fin, 'INVENTORY'] =  acc_data.loc[fin, 'invtq'].fillna(acc_data.loc[fin, 'finivstq'])\n",
    "acc_data.loc[util, 'INVENTORY'] = acc_data.loc[util, 'invtq'].fillna(acc_data.loc[util, 'uinvq'])\n",
    "                                                                     \n",
    "# Current Assets\n",
    "acc_data['CA'] = acc_data['actq']\n",
    "acc_data['CA'] = acc_data['CA'].fillna(acc_data['rectq'] + acc_data['invtq'] + acc_data['cheq'] + acc_data['acoq'])\n",
    "\n",
    "acc_data.loc[fin, 'CA'] = (\n",
    "    acc_data.loc[fin, 'actq']\n",
    "    .fillna(\n",
    "        acc_data.loc[fin, 'rectq'].fillna(acc_data.loc[fin, 'finreccq']) +\n",
    "        acc_data.loc[fin, 'invtq'].fillna(acc_data.loc[fin, 'finivstq']) +\n",
    "        acc_data.loc[fin, 'acoq'].fillna(acc_data.loc[fin, 'finacoq']) +\n",
    "        acc_data.loc[fin, 'cheq'].fillna(acc_data.loc[fin, 'finchq'])\n",
    "    )\n",
    ")\n",
    "\n",
    "acc_data.loc[util, 'CA'] = (\n",
    "    acc_data.loc[util, 'actq']\n",
    "    .fillna(\n",
    "        acc_data.loc[util, 'rectq'].fillna(acc_data.loc[util, 'urectq']) +\n",
    "        acc_data.loc[util, 'invtq'].fillna(acc_data.loc[util, 'uinvq']) +\n",
    "        acc_data.loc[util, 'acoq'].fillna(acc_data.loc[util, 'uacoq']) +\n",
    "        acc_data.loc[util, 'cheq']\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Current Liabilities\n",
    "acc_data['CL'] = acc_data['lctq']\n",
    "acc_data['CL'] = acc_data['CL'].fillna(acc_data['dlcq'] + acc_data['apq'] + acc_data['txpq'] + acc_data['lcoq'])\n",
    "\n",
    "acc_data.loc[fin, 'CL'] = (\n",
    "    acc_data.loc[fin, 'lctq']\n",
    "    .fillna(\n",
    "        acc_data.loc[fin, 'dlcq'].fillna(acc_data.loc[fin, 'findlcq']) +\n",
    "        acc_data.loc[fin, 'apq'].fillna(acc_data.loc[fin, 'finnpq']) +\n",
    "        acc_data.loc[fin, 'lcoq'].fillna(acc_data.loc[fin, 'finlcoq']) +\n",
    "        acc_data.loc[fin, 'txpq']\n",
    "    )\n",
    ")\n",
    "\n",
    "# Operating Current Assets\n",
    "acc_data['COA'] = acc_data['CA'] - acc_data['cheq']\n",
    "acc_data.loc[fin, 'COA'] = acc_data.loc[fin, 'CA']\n",
    "\n",
    "# Operating Current Liabilities\n",
    "acc_data['COL'] = acc_data['CL'] - np.maximum(acc_data['dlcq'], 0)\n",
    "acc_data.loc[fin, 'COL'] = acc_data.loc[fin, 'CL']\n",
    "\n",
    "# Total Assets\n",
    "acc_data['TA'] = acc_data['atq']\n",
    "acc_data['TA'] = acc_data['TA'].fillna(acc_data['seqq'] + acc_data['dlttq'] + np.maximum(acc_data['lctq'], 0))\n",
    "acc_data['TA'] = acc_data['TA'] + acc_data['R&D cap']\n",
    "\n",
    "# Net Operating Assets\n",
    "acc_data['NOA'] = (acc_data['TA'] - acc_data['cheq']) - (acc_data['ltq'] - acc_data['dlcq'] - acc_data['dlttq'])\n",
    "\n",
    "# Value of Preferred Stock \n",
    "acc_data['PSTK'] = acc_data['pstkrv']\n",
    "acc_data['PSTK'] = acc_data['PSTK'].fillna(acc_data['pstkl'])\n",
    "acc_data['PSTK'] = acc_data['PSTK'].fillna(acc_data['pstkq'])\n",
    "acc_data['PSTK'] = acc_data['PSTK'].fillna(0)\n",
    "\n",
    "# Adjusted Book Equity\n",
    "acc_data['BE'] = (acc_data['seqq'] + acc_data['R&D cap']).fillna(acc_data['ceqq'] + acc_data['PSTK'].clip(lower=0) + acc_data['R&D cap'])\n",
    "\n",
    "acc_data.loc[util, 'BE'] = (\n",
    "    (acc_data.loc[util, 'seqq'] + acc_data.loc[util, 'R&D cap'])\n",
    "    .fillna(acc_data.loc[util, 'ceqq'] + acc_data.loc[util, 'PSTK'] + acc_data.loc[util, 'R&D cap'])\n",
    "    .fillna(acc_data.loc[util, 'uceqq'] + acc_data.loc[util, 'PSTK'] + acc_data.loc[util, 'R&D cap'])\n",
    ")\n",
    "\n",
    "# Total Liabilities\n",
    "acc_data['LT'] = acc_data['ltq']\n",
    "\n",
    "# Debt\n",
    "acc_data['STDEBT'] = acc_data['dlcq']\n",
    "acc_data['LTDEBT'] = acc_data['dlttq']\n",
    "\n",
    "acc_data.loc[util, 'LTDEBT'] = acc_data.loc[util, 'dlttq'].fillna(\n",
    "    acc_data.loc[util, 'uddq'] + acc_data.loc[util, 'udmbq'] + acc_data.loc[util, 'udoltq'] + acc_data.loc[util, 'udpcoq']\n",
    ")\n",
    "\n",
    "acc_data['TOTDEBT'] = acc_data['dlttq'] + acc_data['dlcq']\n",
    "\n",
    "# Equity Issuance\n",
    "acc_data['EQIS'] = acc_data['sstky_q']\n",
    "\n",
    "# Minority Interest\n",
    "acc_data['MI'] = np.maximum(acc_data['mibtq'], 0)\n",
    "\n",
    "# Net Working Capital\n",
    "acc_data['NWC'] =  acc_data['wcapq']\n",
    "acc_data['NWC'] = acc_data['NWC'].fillna(acc_data['CA'] - acc_data['CL'])\n",
    "\n",
    "# Current Operating Working Capital\n",
    "acc_data['COWC'] = (acc_data['rectq'] + acc_data['invtq']) - acc_data['apq']\n",
    "acc_data['COWC'] = acc_data['COWC'].fillna(acc_data['NWC'] - acc_data['cheq'] + acc_data['dlcq'])\n",
    "\n",
    "# Non-Current Operating Assets\n",
    "acc_data['ivaoq'] = acc_data['ivaoq'].fillna(0)\n",
    "acc_data['gdwlq'] = acc_data['gdwlq'].fillna(0)\n",
    "acc_data['NCOA'] = acc_data['TA'] - acc_data['CA'] - acc_data['gdwlq'] - np.maximum(acc_data['ivaoq'], 0)\n",
    "\n",
    "# Non-Current Operating Fixed Assets\n",
    "acc_data['intanq'] = acc_data['intanq'].fillna(0)\n",
    "acc_data['NCOFA'] = acc_data['NCOA'] - acc_data['intanq']\n",
    "\n",
    "# Non-Current Operating Liabilities\n",
    "acc_data['NCOL'] = acc_data['ltq'] - acc_data['CL'] - acc_data['dlttq']\n",
    "\n",
    "# Net Debt\n",
    "acc_data['NETDEBT'] = acc_data['TOTDEBT'] - acc_data['cheq']\n",
    "\n",
    "# Net Non-Current Operating Assets\n",
    "acc_data['NNCOA'] = acc_data['NCOA'] - acc_data['NCOL']\n",
    "\n",
    "# Operating Accruals\n",
    "lag_nwc = acc_data.groupby(['PERMNO'])['NWC'].shift(1)\n",
    "lag_nncoa = acc_data.groupby(['PERMNO'])['NNCOA'].shift(1)\n",
    "acc_data['OACC'] = acc_data['NWC'] - lag_nwc + acc_data['NNCOA'] - lag_nncoa\n",
    "\n",
    "# Operating Cash Flow\n",
    "acc_data['OCF'] = acc_data['oancfy_q'] + acc_data['R&D'] - acc_data['R&D_ammort']\n",
    "acc_data['OCF'] = acc_data['OCF'].fillna(acc_data['NI'] - acc_data['OACC'])\n",
    "acc_data['OCF'] = acc_data['OCF'].fillna(acc_data['NI'] + acc_data['dpq'] - np.maximum(acc_data['wcapchy_q'], 0))\n",
    "\n",
    "# Market Capitalization\n",
    "acc_data['MC'] =  acc_data['mkvaltq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_data = pd.read_csv(r'D:\\Master Thesis\\Code\\mkt_data_quarterly_full.csv')\n",
    "mkt_data['quarter'] = mkt_data['quarter'].astype('period[Q]')\n",
    "mkt_data['y'] = mkt_data['quarter'].dt.year\n",
    "mkt_data['qtr'] = mkt_data['quarter'].dt.quarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap Between Market and Fundamental Data; Permnos starting before 1961 or after 2024, who do not have fundamental/market data are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique PERMNOs in market data\n",
    "unique_mkt_permnos = mkt_data['PERMNO'].nunique()\n",
    "print(f\"Unique PERMNOs in market data: {unique_mkt_permnos}\")\n",
    "\n",
    "# Unique PERMNOs in accounting data\n",
    "unique_acc_permnos = acc_data['PERMNO'].nunique()\n",
    "print(f\"Unique PERMNOs in accounting data: {unique_acc_permnos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market data year range\n",
    "mkt_min_year = mkt_data['y'].min()\n",
    "mkt_max_year = mkt_data['y'].max()\n",
    "print(f\"Market data covers years: {mkt_min_year} – {mkt_max_year}\")\n",
    "\n",
    "# Accounting data year range\n",
    "acc_min_year = acc_data['y'].min()\n",
    "acc_max_year = acc_data['y'].max()\n",
    "print(f\"Accounting data covers years: {acc_min_year} – {acc_max_year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define columns to keep\n",
    "accounting_fields = [\n",
    "    'SALES', 'COGS', 'GP', 'OPEX', 'EBITDA', 'D&A', 'EBIT', 'NI',\n",
    "    'INVENTORY', 'CA', 'CL', 'COA', 'COL', 'TA', 'NOA', 'PSTK', 'BE', 'LT',\n",
    "    'STDEBT', 'LTDEBT', 'EQIS', 'MI', 'NWC', 'NCOA', 'COWC', 'CASHST',\n",
    "    'NCOFA', 'NCOL', 'NETDEBT', 'NNCOA', 'OACC', 'OCF'\n",
    "]\n",
    "\n",
    "market_fields = [\n",
    "    'resff3_12_1', 'mom_12_1',\n",
    "    'zero_trades_252', 'zero_trades_21', 'turnover_126d', 'ivol_ff3_21d',\n",
    "    'rmax5_rvol_21d', 'firm_age', 'seas_6_10an', 'seas_6_10na',\n",
    "    'seas_11_15na', 'seas_20_16an'\n",
    "]\n",
    "\n",
    "# Step 2: Subset and keep MC separately\n",
    "keep_cols_mkt = ['PERMNO', 'y', 'qtr', 'RET', 'vwretd', 'mc'] + market_fields\n",
    "keep_cols_acc = ['PERMNO', 'y', 'qtr', 'GIND', 'SIC', 'MC'] + accounting_fields\n",
    "\n",
    "mkt_data = mkt_data[keep_cols_mkt]\n",
    "acc_data = acc_data[keep_cols_acc]\n",
    "\n",
    "# Step 3: Merge\n",
    "data_merged = pd.merge(\n",
    "    mkt_data,\n",
    "    acc_data,\n",
    "    on=['PERMNO', 'y', 'qtr'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Step 4: Fallback MC\n",
    "data_merged['MC'] = data_merged['MC'].fillna(data_merged['mc'] / 1000)\n",
    "data_merged.drop(columns=['mc'], inplace=True)\n",
    "\n",
    "# Step 5: Redefine accounting_fields now that MC is finalized\n",
    "accounting_fields += ['MC']\n",
    "\n",
    "# Merge delay_flag before shifting\n",
    "delay_flags = acc_data_q[['LPERMNO', 'y', 'qtr', 'delay_flag']].rename(columns={'LPERMNO': 'PERMNO'})\n",
    "data_merged = pd.merge(data_merged, delay_flags, on=['PERMNO', 'y', 'qtr'], how='left')\n",
    "data_merged['delay_flag'] = data_merged['delay_flag'].fillna(1).astype(int)  # assume delay if unknown\n",
    "\n",
    "# Step 6: Fluid Shift of accounting fields conditionally (delay_flag controls 1 or 2 lags)\n",
    "for col in accounting_fields:\n",
    "    grouped = data_merged.groupby('PERMNO')[col]\n",
    "    lag1 = grouped.shift(1)\n",
    "    lag2 = grouped.shift(2)\n",
    "    data_merged[col + '_shifted'] = np.where(data_merged['delay_flag'] == 1, lag2, lag1)\n",
    "\n",
    "\n",
    "# Shift market fields uniformly by 1\n",
    "for col in market_fields:\n",
    "    data_merged[col + '_shifted'] = data_merged.groupby('PERMNO')[col].shift(1)\n",
    "\n",
    "data_merged.drop(columns=accounting_fields + market_fields, inplace=True)\n",
    "data_merged.rename(columns={c + '_shifted': c for c in accounting_fields + market_fields}, inplace=True)\n",
    "\n",
    "\n",
    "# Step 7: Final formatting\n",
    "fields = ['PERMNO', 'y', 'qtr', 'GIND', 'SIC', 'RET', 'vwretd'] + accounting_fields + market_fields\n",
    "data_merged = data_merged[fields].sort_values(['PERMNO', 'y', 'qtr'])\n",
    "\n",
    "# Export\n",
    "data_merged.to_csv('data/data_merged_fluidshift.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final merged shape: {data_merged.shape}\")\n",
    "print(f\"Unique firms: {data_merged['PERMNO'].nunique()}\")\n",
    "print(f\"Years covered: {data_merged['y'].min()} to {data_merged['y'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift Sanity Check; Market Data Shifted by 1 row, Accounting Data Shifted by 2 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a common PERMNO\n",
    "permno = data_merged['PERMNO'].dropna().unique()[0]\n",
    "\n",
    "# Sort and subset raw datasets\n",
    "acc_sample = acc_data[acc_data['PERMNO'] == permno].sort_values(['y', 'qtr'])[['y', 'qtr', 'SALES']]\n",
    "mkt_sample = mkt_data[mkt_data['PERMNO'] == permno].sort_values(['y', 'qtr'])[\n",
    "    ['y', 'qtr', 'RET', 'vwretd', 'ivol_ff3_21d', 'zero_trades_21']\n",
    "]\n",
    "merged_sample = data_merged[data_merged['PERMNO'] == permno].sort_values(['y', 'qtr'])[\n",
    "    ['y', 'qtr', 'SALES', 'RET', 'vwretd', 'ivol_ff3_21d', 'zero_trades_21']\n",
    "]\n",
    "\n",
    "# Rename for clarity\n",
    "acc_sample = acc_sample.rename(columns={'SALES': 'SALES_raw'})\n",
    "mkt_sample = mkt_sample.rename(columns={\n",
    "    'RET': 'RET_raw', \n",
    "    'vwretd': 'vwretd_raw',\n",
    "    'ivol_ff3_21d': 'ivol_ff3_21d_raw',\n",
    "    'zero_trades_21': 'zero_trades_21_raw'\n",
    "})\n",
    "merged_sample = merged_sample.rename(columns={\n",
    "    'SALES': 'SALES_shifted',\n",
    "    'RET': 'RET_current',\n",
    "    'vwretd': 'vwretd_current',\n",
    "    'ivol_ff3_21d': 'ivol_ff3_21d_shifted',\n",
    "    'zero_trades_21': 'zero_trades_21_shifted'\n",
    "})\n",
    "\n",
    "# Merge all for comparison\n",
    "check = pd.merge(merged_sample, acc_sample, on=['y', 'qtr'], how='left')\n",
    "check = pd.merge(check, mkt_sample, on=['y', 'qtr'], how='left')\n",
    "\n",
    "# Show results\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(check.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
